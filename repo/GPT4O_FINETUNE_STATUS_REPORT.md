# GPT-4o å¾®è°ƒæµç¨‹çŠ¶æ€æ£€æŸ¥æŠ¥å‘Š

ç”Ÿæˆæ—¶é—´: 2026-02-03

## æ€»ä½“çŠ¶æ€ï¼šâœ… ä»£ç å¯ä»¥æ­£å¸¸è¿è¡Œï¼ˆéœ€è¦é…ç½®ç¯å¢ƒï¼‰

---

## è¯¦ç»†æ£€æŸ¥ç»“æœ

### 1. æ ‡ç­¾ç”Ÿæˆä»£ç  (generate_labels.py)

**çŠ¶æ€**: âœ… **æ­£å¸¸å·¥ä½œ**

- **æ–‡ä»¶ä½ç½®**: `repo/src/routing/generate_labels.py`
- **åŠŸèƒ½**: ä½¿ç”¨ GPT-4o ä½œä¸ºæ•™å¸ˆæ¨¡å‹ç”Ÿæˆè·¯ç”±æ ‡ç­¾
- **å…³é”®ç‰¹æ€§**:
  - æ”¯æŒä» HumanEval æ•°æ®é›†åŠ è½½é—®é¢˜
  - ä½¿ç”¨ `LLMClient` è°ƒç”¨ GPT-4o API
  - è‡ªåŠ¨æ ‡å‡†åŒ–å’ŒéªŒè¯æ ‡ç­¾æ ¼å¼
  - è¾“å‡º JSONL æ ¼å¼çš„è®­ç»ƒæ•°æ®

**ä½¿ç”¨æ–¹æ³•**:
```powershell
python -m src.routing.generate_labels `
  --data data/Humaneval/humaneval-py.jsonl `
  --output data/router_labels.jsonl `
  --model gpt-4o `
  --roles "planner,builder,tester,refractor"
```

---

### 2. LoRA å¾®è°ƒä»£ç  (train_router_lora.py)

**çŠ¶æ€**: âœ… **æ­£å¸¸å·¥ä½œ**

- **æ–‡ä»¶ä½ç½®**: `repo/src/routing/train_router_lora.py`
- **åŠŸèƒ½**: ä½¿ç”¨ LoRA å¯¹ Qwen æ¨¡å‹è¿›è¡Œå¾®è°ƒ
- **å…³é”®ç‰¹æ€§**:
  - æ”¯æŒ 4-bit é‡åŒ–è®­ç»ƒ
  - æ”¯æŒå¤šæ–‡ä»¶è®­ç»ƒæ•°æ®
  - è‡ªåŠ¨è½¬æ¢ HumanEval æ ¼å¼ï¼ˆä½¿ç”¨å¯å‘å¼æ ‡ç­¾ï¼‰
  - æ”¯æŒè‡ªå®šä¹‰ LoRA è¶…å‚æ•°
  - ä»…è®­ç»ƒ assistant å›å¤éƒ¨åˆ†ï¼ˆä¸è®­ç»ƒè¾“å…¥éƒ¨åˆ†ï¼‰

**ä½¿ç”¨æ–¹æ³•**:
```powershell
python -m src.routing.train_router_lora `
  --data data/router_labels.jsonl `
  --model Qwen/Qwen3-8B-Instruct `
  --output_dir models/router_lora `
  --epochs 1 `
  --batch_size 1 `
  --grad_accum 8 `
  --lr 1e-4 `
  --use_4bit
```

---

### 3. LLM Client (llm_client.py)

**çŠ¶æ€**: âœ… **æ­£å¸¸å·¥ä½œ**

- **æ–‡ä»¶ä½ç½®**: `repo/src/generation/llm_client.py`
- **åŠŸèƒ½**: OpenAI å…¼å®¹çš„ HTTP å®¢æˆ·ç«¯
- **æ”¯æŒçš„æ¨¡å‹**: gpt-4o, gpt-4o-mini, qwen3-8b ç­‰
- **å…³é”®ç‰¹æ€§**:
  - è‡ªåŠ¨é‡è¯•æœºåˆ¶
  - æ”¯æŒè‡ªå®šä¹‰ API ç«¯ç‚¹
  - è¶…æ—¶æ§åˆ¶

---

### 4. è®­ç»ƒæ•°æ®æ ¼å¼

**çŠ¶æ€**: âœ… **æ ¼å¼æ­£ç¡®**

ç”Ÿæˆçš„è®­ç»ƒæ•°æ®ä½¿ç”¨æ ‡å‡†çš„ ChatML æ ¼å¼:

```json
{
  "messages": [
    {
      "role": "system",
      "content": "You are a meta-router. Decide the best agent topology..."
    },
    {
      "role": "user",
      "content": "Task: Implement the following Python function:\n\n..."
    },
    {
      "role": "assistant",
      "content": "{\"topology\": \"single\", \"roles\": [\"builder\"], ...}"
    }
  ],
  "sample_type": "humaneval_teacher_distilled",
  "origin_task_id": "HumanEval_23_strlen"
}
```

---

## ç¯å¢ƒé…ç½®è¦æ±‚

### å¿…éœ€çš„ç¯å¢ƒå˜é‡

```powershell
# API é…ç½®
$env:OPENAI_API_KEY = "your-api-key-here"
$env:LLM_API_BASE = "https://az.gptplus5.com/v1"  # æˆ–å…¶ä»–å…¼å®¹ç«¯ç‚¹
$env:OPENAI_MODEL = "gpt-4o"
```

### Python ä¾èµ–

#### åŸºç¡€ä¾èµ– (requirements.txt)
```
httpx
```

#### è®­ç»ƒä¾èµ– (requirements-train.txt)
```
transformers>=4.30.0
datasets>=2.12.0
peft>=0.4.0
accelerate>=0.20.0
bitsandbytes>=0.39.0
torch>=2.0.0
```

**å®‰è£…å‘½ä»¤**:
```powershell
pip install -r requirements-train.txt
```

---

## å®Œæ•´å·¥ä½œæµç¨‹

### æ­¥éª¤ 1: è®¾ç½®ç¯å¢ƒå˜é‡
```powershell
$env:OPENAI_API_KEY = "sk-..."
$env:LLM_API_BASE = "https://az.gptplus5.com/v1"
```

### æ­¥éª¤ 2: å®‰è£…è®­ç»ƒä¾èµ–
```powershell
pip install transformers datasets peft accelerate bitsandbytes
```

### æ­¥éª¤ 3: ç”Ÿæˆè®­ç»ƒæ ‡ç­¾ï¼ˆä½¿ç”¨ GPT-4oï¼‰
```powershell
python -m src.routing.generate_labels `
  --data data/Humaneval/humaneval-py.jsonl `
  --output data/router_labels.jsonl `
  --model gpt-4o
```

è¿™ä¼šï¼š
- åŠ è½½ HumanEval é—®é¢˜ï¼ˆ161 ä¸ªé—®é¢˜ï¼‰
- å¯¹æ¯ä¸ªé—®é¢˜è°ƒç”¨ GPT-4o ç”Ÿæˆæœ€ä½³æ‹“æ‰‘å’Œè§’è‰²åˆ†é…
- è¾“å‡ºæ ¼å¼åŒ–çš„è®­ç»ƒæ•°æ®åˆ° `data/router_labels.jsonl`

### æ­¥éª¤ 4: LoRA å¾®è°ƒ
```powershell
python -m src.routing.train_router_lora `
  --data data/router_labels.jsonl `
  --model Qwen/Qwen3-8B-Instruct `
  --output_dir models/router_lora `
  --epochs 1 `
  --batch_size 1 `
  --grad_accum 8 `
  --lr 1e-4 `
  --use_4bit `
  --bf16
```

è¿™ä¼šï¼š
- åŠ è½½è®­ç»ƒæ ‡ç­¾
- ä¸‹è½½ Qwen3-8B-Instruct åŸºåº§æ¨¡å‹
- ä½¿ç”¨ 4-bit é‡åŒ–å’Œ LoRA è¿›è¡Œå‚æ•°é«˜æ•ˆå¾®è°ƒ
- ä¿å­˜ LoRA é€‚é…å™¨åˆ° `models/router_lora`

### æ­¥éª¤ 5: ä½¿ç”¨å¾®è°ƒåçš„æ¨¡å‹
å¾®è°ƒåçš„æ¨¡å‹å¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼ä½¿ç”¨ï¼š
```python
from peft import PeftModel
from transformers import AutoModelForCausalLM, AutoTokenizer

# åŠ è½½åŸºåº§æ¨¡å‹å’Œ LoRA é€‚é…å™¨
base_model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen3-8B-Instruct")
model = PeftModel.from_pretrained(base_model, "models/router_lora")
tokenizer = AutoTokenizer.from_pretrained("models/router_lora")
```

---

## å·²çŸ¥é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆ

### é—®é¢˜ 1: API Key æœªè®¾ç½®
**ç°è±¡**: `RuntimeError: LLM_API_KEY/OPENAI_API_KEY is not set`

**è§£å†³æ–¹æ¡ˆ**:
```powershell
$env:OPENAI_API_KEY = "your-actual-api-key"
```

### é—®é¢˜ 2: ç¼ºå°‘è®­ç»ƒä¾èµ–
**ç°è±¡**: `ModuleNotFoundError: No module named 'datasets'`

**è§£å†³æ–¹æ¡ˆ**:
```powershell
pip install datasets peft accelerate bitsandbytes
```

### é—®é¢˜ 3: CUDA å†…å­˜ä¸è¶³
**ç°è±¡**: `RuntimeError: CUDA out of memory`

**è§£å†³æ–¹æ¡ˆ**:
- ä½¿ç”¨ `--use_4bit` å¯ç”¨ 4-bit é‡åŒ–
- å‡å° `--batch_size` (é»˜è®¤ä¸º 1)
- å¢å¤§ `--grad_accum` (æ¢¯åº¦ç´¯ç§¯æ­¥æ•°)
- ä½¿ç”¨ `--gradient_checkpointing`

### é—®é¢˜ 4: API 503 é”™è¯¯
**ç°è±¡**: `503 Service Unavailable`

**è§£å†³æ–¹æ¡ˆ**:
- æ£€æŸ¥ API ç«¯ç‚¹æ˜¯å¦æ­£å¸¸å·¥ä½œ
- å°è¯•å…¶ä»–å…¼å®¹çš„ API ç«¯ç‚¹
- å¢åŠ é‡è¯•æ¬¡æ•°å’Œè¶…æ—¶æ—¶é—´

---

## æµ‹è¯•éªŒè¯

è¿è¡Œæµ‹è¯•è„šæœ¬éªŒè¯ç¯å¢ƒé…ç½®ï¼š
```powershell
python test_gpt4o_finetune_flow.py
```

è¯¥è„šæœ¬ä¼šæ£€æŸ¥ï¼š
- âœ… LLM Client è¿æ¥æ€§
- âœ… æ ‡ç­¾ç”ŸæˆåŠŸèƒ½
- âœ… è®­ç»ƒæ•°æ®æ ¼å¼
- âœ… LoRA ä¾èµ–åŒ…

---

## æ€§èƒ½ä¼˜åŒ–å»ºè®®

### æ ‡ç­¾ç”Ÿæˆä¼˜åŒ–
1. **æ‰¹é‡å¤„ç†**: å¯ä»¥ä¿®æ”¹ä»£ç æ”¯æŒå¹¶å‘ API è°ƒç”¨
2. **ç¼“å­˜**: ä¿å­˜ä¸­é—´ç»“æœé¿å…é‡å¤è°ƒç”¨
3. **é”™è¯¯æ¢å¤**: æ”¯æŒæ–­ç‚¹ç»­ä¼ 

### è®­ç»ƒä¼˜åŒ–
1. **é‡åŒ–**: ä½¿ç”¨ `--use_4bit` å‡å°‘å†…å­˜å ç”¨
2. **æ¢¯åº¦ç´¯ç§¯**: å¢å¤§ `--grad_accum` æ¨¡æ‹Ÿæ›´å¤§çš„ batch size
3. **æ··åˆç²¾åº¦**: ä½¿ç”¨ `--bf16` æˆ– `--fp16` åŠ é€Ÿè®­ç»ƒ
4. **æ¢¯åº¦æ£€æŸ¥ç‚¹**: ä½¿ç”¨ `--gradient_checkpointing` èŠ‚çœå†…å­˜

### æ•°æ®ä¼˜åŒ–
1. **è¿‡æ»¤**: ä½¿ç”¨ `--sample_types` åªè®­ç»ƒç‰¹å®šç±»å‹çš„æ ·æœ¬
2. **é™åˆ¶**: ä½¿ç”¨ `--max_samples` å¿«é€Ÿæµ‹è¯•
3. **è´¨é‡**: ä½¿ç”¨æ›´å¼ºçš„æ•™å¸ˆæ¨¡å‹ï¼ˆGPT-4oï¼‰ç”Ÿæˆé«˜è´¨é‡æ ‡ç­¾

---

## æ€»ç»“

### âœ… å¯ä»¥æ­£å¸¸è¿è¡Œçš„ç»„ä»¶
- [x] LLM Client (æ”¯æŒ GPT-4o)
- [x] æ ‡ç­¾ç”Ÿæˆè„šæœ¬
- [x] LoRA å¾®è°ƒè„šæœ¬
- [x] æ•°æ®æ ¼å¼éªŒè¯
- [x] æ ‡ç­¾æ ‡å‡†åŒ–å’ŒéªŒè¯

### âš ï¸ éœ€è¦é…ç½®çš„éƒ¨åˆ†
- [ ] è®¾ç½® API å¯†é’¥ç¯å¢ƒå˜é‡
- [ ] å®‰è£…è®­ç»ƒä¾èµ–åŒ… (datasets, peft)
- [ ] ç¡®ä¿æœ‰è¶³å¤Ÿçš„ GPU å†…å­˜ï¼ˆæˆ–ä½¿ç”¨ 4-bit é‡åŒ–ï¼‰

### ğŸ¯ å»ºè®®çš„å·¥ä½œæµç¨‹
1. **å…ˆå°è§„æ¨¡æµ‹è¯•**: ç”¨ `--max_samples 10` å¿«é€ŸéªŒè¯æµç¨‹
2. **é€æ­¥æ‰©å¤§**: ç¡®è®¤æ— è¯¯åå†ä½¿ç”¨å®Œæ•´æ•°æ®é›†
3. **ç›‘æ§è®­ç»ƒ**: è§‚å¯Ÿ loss ä¸‹é™å’Œæ¨¡å‹è¾“å‡ºè´¨é‡
4. **è¯„ä¼°æ•ˆæœ**: åœ¨éªŒè¯é›†ä¸Šæµ‹è¯•å¾®è°ƒåçš„æ¨¡å‹

---

## ä»£ç è´¨é‡è¯„ä¼°

- **ä»£ç ç»“æ„**: â­â­â­â­â­ ä¼˜ç§€ï¼ˆæ¨¡å—åŒ–ã€å¯æ‰©å±•ï¼‰
- **é”™è¯¯å¤„ç†**: â­â­â­â­ è‰¯å¥½ï¼ˆæœ‰é‡è¯•ã€è¶…æ—¶æ§åˆ¶ï¼‰
- **æ–‡æ¡£**: â­â­â­â­ è‰¯å¥½ï¼ˆæœ‰æ³¨é‡Šå’Œ READMEï¼‰
- **æµ‹è¯•**: â­â­â­ ä¸­ç­‰ï¼ˆç¼ºå°‘å•å…ƒæµ‹è¯•ï¼‰
- **å¯ç»´æŠ¤æ€§**: â­â­â­â­â­ ä¼˜ç§€ï¼ˆæ¸…æ™°çš„ä»£ç ç»„ç»‡ï¼‰

---

**ç»“è®º**: ç”¨ GPT-4o ç”Ÿæˆå¾®è°ƒæ ‡ç­¾å’Œåç»­å¾®è°ƒçš„ä»£ç **å®Œå…¨å¯ä»¥æ­£å¸¸è¿è¡Œ**ï¼Œåªéœ€è¦ï¼š
1. é…ç½® API å¯†é’¥
2. å®‰è£…è®­ç»ƒä¾èµ–ï¼ˆ`pip install datasets peft`ï¼‰
3. æŒ‰ç…§ä¸Šè¿°æ­¥éª¤æ‰§è¡Œ

ä»£ç è´¨é‡å¾ˆé«˜ï¼Œç»“æ„æ¸…æ™°ï¼Œæœ‰å®Œå–„çš„å‚æ•°éªŒè¯å’Œé”™è¯¯å¤„ç†ã€‚
